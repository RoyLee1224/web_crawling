{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "35bedeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "dba299c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"/Users/lizhechen/Downloads/Programming/korean\" #<-- absolute dir the script\n",
    "# script_dir = os.path.dirname(file_name) \n",
    "os.chdir(file_dir)\n",
    "if not(os.path.exists(file_name+\"/\"+'pdf')):\n",
    "    os.mkdir('pdf')\n",
    "if not(os.path.exists(file_name+\"/\"+'txt')):\n",
    "    os.mkdir('txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a1180a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTML(url):\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.text,\"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "32adc961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################create 'thema.txt' from '테마여행'##############################\n",
      "thema.txt downloaded\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*30 + \"create 'thema.txt' from '테마여행'\" + \"#\"*30)\n",
    "\n",
    "url_bbs = 'https://www.taiwantour.or.kr/bbs'\n",
    "url_thema = \"https://www.taiwantour.or.kr/bbs/board.php?bo_table=m08_01&sca=%ED%9C%B4%EC%96%91\" #휴양\n",
    "a_list = getHTML(url_thema).select('div.sub_menu a')\n",
    "thema_content = \"\"\n",
    "\n",
    "for a in a_list:\n",
    "    url_bbs_subPage = url_bbs + a['href'][1:]\n",
    "    tags = getHTML(url_bbs_subPage).select('div.rt p')\n",
    "    for tag in tags:\n",
    "        thema_content += tag.text\n",
    "\n",
    "# print(thema_content)\n",
    "f= open(\"thema.txt\",\"w+\")\n",
    "f.write(thema_content)\n",
    "f.close()\n",
    "\n",
    "print(\"thema.txt downloaded\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d107f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#\"*30+\" create 'pro.txt' from '프로대만족'\" + \"#\"*30)\n",
    "\n",
    "url_pro = 'https://www.taiwantour.or.kr/bbs/board.php?bo_table=m06_12'\n",
    "pro_content = \"\"\n",
    "\n",
    "\n",
    "def get_pro_content(url):\n",
    "    global pro_content\n",
    "    a_list = getHTML(url).select('div.subject a')\n",
    "    for a in a_list:\n",
    "        p_list = getHTML(a[\"href\"]).select('div.magazine_view p')\n",
    "\n",
    "        for p in p_list:\n",
    "            pro_content += p.text\n",
    "\n",
    "def get_all_content(url):\n",
    "    global place_content\n",
    "    dis_list = getHTML(url).select('div.list-details')\n",
    "    for dis in dis_list:\n",
    "        place_content += dis.text\n",
    "\n",
    "def get_pro_page(pages):\n",
    "    temp_url = url_pro\n",
    "    for page in range(1,pages):\n",
    "        print(f\"downloading page {page}\")\n",
    "        next_page_url = url_pro + \"&page=\" + str(page)\n",
    "        temp_url = next_page_url\n",
    "        get_pro_content(temp_url)\n",
    "        \n",
    "\n",
    "get_pro_page(21)\n",
    "\n",
    "f= open(\"pro.txt\",\"w+\")\n",
    "f.write(pro_content)\n",
    "f.close()\n",
    "\n",
    "print(\"pro.txt downloaded\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "d781a6dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################create 'place.txt' from '대만 명소'##############################\n",
      "downloading page 1\n",
      "downloading page 2\n",
      "downloading page 3\n",
      "downloading page 4\n",
      "downloading page 5\n",
      "downloading page 6\n",
      "downloading page 7\n",
      "downloading page 8\n",
      "place.txt downloaded\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*30 + \"create 'place.txt' from '대만 명소'\" + \"#\"*30)\n",
    "\n",
    "url_place = 'https://www.taiwantour.or.kr/bbs/board.php?bo_table=m03&'\n",
    "place_content = \"\"\n",
    "\n",
    "url_content=\"https://www.taiwantour.or.kr/bbs/board.php?bo_table=m03&wr_id=63\"\n",
    "\n",
    "def get_content(url):\n",
    "    global place_content\n",
    "    view_content_list = getHTML(url).select('div.view-content')\n",
    "    content_list = getHTML(url).select('div.content')   \n",
    "    all_content_list = view_content_list + content_list\n",
    "    \n",
    "    for all_content in all_content_list:\n",
    "        contents = all_content.contents\n",
    "        title = contents[0].parent.parent.h1 or contents[0].parent.parent.div\n",
    "        place_content =  place_content + \"\\n\" + title.text + \"\\n\"\n",
    "#         if there's more than one block in description, the first sentence will disappear due to unknown reason.\n",
    "#         so the first sentence have to be handled separately\n",
    "        if(len(contents) != 1 and (\"\\n\" not in contents)):\n",
    "            first_sentence = contents[0]\n",
    "            place_content += first_sentence\n",
    "        \n",
    "        for sentence in contents:\n",
    "#             remove Tags such as <br/> \n",
    "            if str(type(sentence)) !=\"<class 'bs4.element.Tag'>\"  :\n",
    "                place_content += sentence\n",
    "        \n",
    "#     print(\"place_content\", place_content)\n",
    "\n",
    "def get_links(url):\n",
    "    a_list = getHTML(url).select('div.list-link a')\n",
    "    for a in a_list:\n",
    "        get_content(a[\"href\"])\n",
    "\n",
    "def get_place_dis(url):\n",
    "    global place_content\n",
    "    dis_list = getHTML(url).select('div.list-details')\n",
    "    for dis in dis_list:\n",
    "        place_content += dis.text\n",
    "\n",
    "def get_place_page(pages):\n",
    "    temp_url = url_place\n",
    "    for page in range(1,pages):\n",
    "        print(f\"downloading page {page}\")\n",
    "        next_page_url = url_place + \"page=\" + str(page)\n",
    "        temp_url = next_page_url\n",
    "        get_links(temp_url)\n",
    "\n",
    "get_place_page(9)\n",
    "\n",
    "# print(place_content)\n",
    "\n",
    "f= open(\"place.txt\",\"w+\")\n",
    "f.write(place_content)\n",
    "f.close()\n",
    "\n",
    "print(\"place.txt downloaded\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "2bae426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################download quarterly magazine PDF##############################\n",
      "Search for:  ['Vol. 44', 'Vol. 45', 'Vol. 46']\n",
      "Downloading file:  v46.pdf\n",
      "Downloading file:  v45.pdf\n",
      "Downloading file:  v44.pdf\n",
      "All PDF files downloaded\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*30+\"download quarterly magazine PDF\"+\"#\"*30)\n",
    "\n",
    "url_magazine = \"https://www.tva.org.tw/Publications?\"\n",
    "url_start_page = \"https://www.tva.org.tw/Publications?page=1\"\n",
    "span_list = getHTML(url_magazine).select('ul.text li span')\n",
    "target_list = []\n",
    "download_list = []\n",
    "\n",
    "def get_magazine_page(pages):\n",
    "    global span_list\n",
    "    temp_url = url_start_page\n",
    "    for page in range(2,pages):\n",
    "        span_list += getHTML(temp_url).select('ul.text li span')\n",
    "        next_page_url = url_magazine + \"page=\" + str(page)\n",
    "        temp_url = next_page_url\n",
    "\n",
    "get_magazine_page(21)\n",
    "\n",
    "for i in range(44,47):\n",
    "    target_list.append(\"Vol. \" + str(i))\n",
    "\n",
    "print(\"Search for: \",target_list)\n",
    "\n",
    "for span in span_list:\n",
    "#     if \"Vol.\" in span.text:\n",
    "    if span.text.strip() in target_list:\n",
    "        name = span.text.strip()\n",
    "        a_tag = span.parent.previous_sibling.previous_sibling[\"href\"]\n",
    "        number = name.split(' ')[-1]\n",
    "        url_pdf = f\"https://www.tva.org.tw{a_tag}\"\n",
    "        \n",
    "#         print(f\"{name} https://www.tva.org.tw{a_tag}\")\n",
    "#         print(f\"v{number}.pdf\")\n",
    "        print(\"Downloading file: \", f\"v{number}.pdf\")\n",
    "\n",
    "        # Get response object for link\n",
    "        response = requests.get(url_pdf)\n",
    "        \n",
    "        \n",
    "        # Write content in pdf file\n",
    "        os.chdir(file_dir)\n",
    "        pdf = open(os.path.join(file_dir,\"pdf\", f\"v{number}.pdf\"), 'wb')\n",
    "        pdf.write(response.content)\n",
    "        pdf.close()\n",
    "\n",
    "print(\"All PDF files downloaded\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "5a37288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################convert pdf to txt##############################\n",
      "Converting v44.pdf\n",
      "Converting v45.pdf\n",
      "Converting v46.pdf\n",
      "All pdf files converted\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*30+\"convert pdf to txt\"+\"#\"*30)\n",
    "\n",
    "for target in target_list:\n",
    "    number = target.split(' ')[-1]\n",
    "    print(f\"Converting v{number}.pdf\")\n",
    "    os.chdir(os.path.join(file_dir,\"pdf\"))\n",
    "    text  = extract_text(f\"v{number}.pdf\", 'rb')\n",
    "    f= open(os.path.join(file_dir,\"txt\",f\"v{number}.txt\"),\"w+\")\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "\n",
    "print(\"All pdf files converted\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "df6f9c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################merge txt##############################\n",
      "44~46.txt downloaded\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*30+\"merge txt\"+\"#\"*30)\n",
    "\n",
    "filenames = []\n",
    "for target in target_list:\n",
    "    number = target.split(' ')[-1]\n",
    "    filenames.append(f\"v{number}.txt\")\n",
    "\n",
    "os.chdir(file_dir)\n",
    "with open('44~46.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(os.path.join(file_dir,\"txt\",fname)) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "print(\"44~46.txt downloaded\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
